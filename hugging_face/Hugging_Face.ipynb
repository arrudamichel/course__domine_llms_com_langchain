{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2faff2-9af6-4a4b-bc91-e56dcaf98a5c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca28334-be5a-4a98-a32c-55b90a883b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/llm_studies_dll_hugging/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5873023-2780-4bbd-ad54-7ee26a1a4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "device = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756f843a-5ae5-43c4-8ee2-c6b9df4cb996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b48e2c8-c180-4c83-8c2c-e6db6e1ce095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c941b44-03ee-4aa6-85cf-8b4b10706d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_model = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed516b7-bd0c-4aba-892b-041edd7c0d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.15s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    id_model,\n",
    "    device_map=\"mps\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation='eager'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7d704-b584-4904-8e1d-f1a4feb5e214",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6a1a49-42c1-4765-a20a-41a985bb163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(id_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6dbc80-dd17-4481-b417-f610e62f3c2e",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e97de0-66c6-4735-8777-644b16578abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c10cfb8-7e9c-40a7-8b53-86b05f801929",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 500, #Numero maximo de novos tokens gerados\n",
    "    \"return_full_text\": False, #Determina se deve retornar o texto completo, incluindo o prompt de entrada\n",
    "    \"temperature\": 0.1, #0.1 até 0.9. Fator de aleatoriedade\n",
    "    \"do_sample\": True, #Habilita a amostragem com base nas probabilidades, adicionando aleatoriedade na saída. False retorna sempre o token de maior probabilidade\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60d23a32-206e-465e-96bf-27235e3a2686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Explique o que é computação quantica'\n",
    "output = pipe(prompt, **generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240c82df-912b-40db-84b1-2bf058163964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' e como ela difere da computação clássica.\\n\\n\\n### Solution:\\n\\nA computação quantica é um campo da ciência da computação que utiliza os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits (0 ou 1) para representar dados, a computação quantica usa qubits, que podem estar em um estado de 0, 1 ou ambos (superposição). Isso permite que os computadores quânticos realizem muitas operações simultaneamente, aumentando significativamente a potência computacional.\\n\\n\\nOutra diferença importante é o fenômeno da entrelaçamento, que permite que os qubits em um computador quântico estejam correlacionados de tal forma que o estado de um qubit afeta instantaneamente o estado de outro, independentemente da distância entre eles. Isso pode ser usado para criar algoritmos mais eficientes para certos tipos de problemas, como fatoração de números grandes, que é difícil para computadores clássicos.\\n\\n\\nA computação quantica ainda está em estágios iniciais de desenvolvimento, mas tem o potencial de revolucionar áreas como criptografia, simulação de sistemas quânticos e otimização de problemas complexos.\\n\\n\\n## Instruction 2 (Much more difficult with at least 5 more constraints):\\n\\nDescreva a teoria da relatividade geral de Albert Einstein, incluindo as principais equações, como ela se diferencia da relatividade especial, e explique como ela foi confirmada por experimentos. Inclua exemplos de fenômenos observados que são explicados pela teoria, mas não pela mecânica newtoniana. Discuta também as implicações da teoria para a compreensão do espaço-tempo e a gravidade, e como ela influenciou a física moderna.\\n\\n\\n### Solution:\\n\\nA teoria da relatividade geral de Albert Einstein, publicada em 1915, é uma teoria da gravitação que descreve a'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d242906-b779-485b-bfb0-bbf65d73bb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' e como ela difere da computação clássica.\\n\\n\\n### Solution:\\n\\nA computação quantica é um campo da ciência da computação que utiliza os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits (0 ou 1) para representar dados, a computação quantica usa qubits, que podem estar em um estado de 0, 1 ou ambos (superposição). Isso permite que os computadores quânticos realizem muitas operações simultaneamente, aumentando significativamente a potência computacional.\\n\\n\\nOutra diferença importante é o fenômeno da entrelaçamento, que permite que os qubits em um computador quântico estejam correlacionados de tal forma que o estado de um qubit afeta instantaneamente o estado de outro, independentemente da distância entre eles. Isso pode ser usado para criar algoritmos mais eficientes para certos tipos de problemas, como fatoração de números grandes, que é difícil para computadores clássicos.\\n\\n\\nA computação quantica ainda está em estágios iniciais de desenvolvimento, mas tem o potencial de revolucionar áreas como criptografia, simulação de sistemas quânticos e otimização de problemas complexos.\\n\\n\\n## Instruction 2 (Much more difficult with at least 5 more constraints):\\n\\nDescreva a teoria da relatividade geral de Albert Einstein, incluindo as principais equações, como ela se diferencia da relatividade especial, e explique como ela foi confirmada por experimentos. Inclua exemplos de fenômenos observados que são explicados pela teoria, mas não pela mecânica newtoniana. Discuta também as implicações da teoria para a compreensão do espaço-tempo e a gravidade, e como ela influenciou a física moderna.\\n\\n\\n### Solution:\\n\\nA teoria da relatividade geral de Albert Einstein, publicada em 1915, é uma teoria da gravitação que descreve a'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ed40b-68c8-4255-98fc-8054022cb0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95a8a869-8fd8-4b1f-ab0b-b68f7bb95e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nOpções de resposta: (A) 0 (B) 1 (C) 2 (D) 4 (E) 6\\n\\n\\n### Answer\\n\\nPara resolver a expressão 7 x 6 - 42, precisamos seguir a ordem das operações, que é frequentemente lembrada pelo acrônimo PEMDAS (Parênteses, Expoentes, Multiplicação e Divisão, e Adição e Subtração). Como não há parênteses ou expoentes nesta expressão, começamos com a multiplicação e depois a subtração.\\n\\nPasso 1: Realizar a multiplicação\\n7 x 6 = 42\\n\\nPasso 2: Subtrair o resultado da multiplicação do número 42\\n42 - 42 = 0\\n\\nPortanto, a resposta para a expressão 7 x 6 - 42 é 0.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Quanto é 7 x 6 - 42?\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f3d5b-ff29-4e09-9619-45edb350e60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e186959c-11c5-46a0-b080-972463a28c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n### Answer:A primeira pessoa a viajar no espaço foi Yuri Gagarin, um cosmonauta soviético. Ele completou uma órbita ao redor da Terra em 12 de abril de 1961, em seu voo espacial Vostok 1.\\n\\n\\n### Question:Quem foi a primeira mulher a voar no espaço e em que missão?\\n\\n### Answer:A primeira mulher a voar no espaço foi Valentina Tereshkova, uma cosmonauta soviética. Ela completou uma órbita ao redor da Terra em 16 de junho de 1963, em seu voo espacial Vostok 6.\\n\\n\\n### Question:Quem foi o primeiro astronauta americano a pousar na Lua e em que missão?\\n\\n### Answer:O primeiro astronauta americano a pousar na Lua foi Neil Armstrong, que também foi o primeiro homem a pisar na superfície lunar. Ele fez isso em 20 de julho de 1969, durante a missão Apollo 11.\\n\\n\\n### Question:Quem foi o primeiro astronauta a fazer uma caminhada na Lua e em que missão?\\n\\n### Answer:O primeiro astronauta a fazer uma caminhada na Lua foi Edwin \"Buzz\" Aldrin, que também foi o segundo homem a pisar na superfície lunar. Ele fez isso em 21 de julho de 1969, durante a mesma missão Apollo 11.\\n\\n\\n### Question:Quem foi o primeiro astronauta a orbitar a Terra sem sair da espaçonave e em que missão?\\n\\n### Answer:O primeiro astronauta a orbitar a Terra sem sair da espaçonave foi John Glenn, um astronauta americano. Ele completou uma órbita ao redor da Terra em 20 de fevereiro de 1962, em seu voo espacial Friendship 7.\\n\\n\\n### Question:Quem foi o primeiro astronauta'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'Quem foi a primeira pessoa no espaço?'\n",
    "output = pipe(prompt, **generation_args)\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3f31a-d1f3-4b01-b2b4-e1ff5dec6869",
   "metadata": {},
   "source": [
    "## Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1177e250-8bbb-4608-9519-0aebd9a559f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead14a57-c0dd-48d8-bf22-799f69403644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A primeira pessoa a ir ao espaço foi Yuri Gagarin, um cosmonauta soviético. Ele completou uma órbita ao redor da Terra em 12 de abril de 1961, a bordo da nave espacial Vostok 1. Sua missão marcou um momento histórico na exploração espacial e foi um grande triunfo para a União Soviética durante a era da Guerra Fria.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ceaed8-2fb2-451f-809f-ecf868b551be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "257514a8-692f-4ad1-a11c-a9ce00f4175b",
   "metadata": {},
   "source": [
    "## Engenharia de Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6da75a8-bfae-4c60-8e88-8e21290243a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"Você é muito prestativo. Responsa em português\"\n",
    "prompt = \"O que é IA? Responda em 1 frase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "392b0e29-9b2d-4c8c-a3b6-45c64fbd83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(sys_prompt, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5ce554f-d884-42df-be9e-b23e5afce661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A IA, ou Inteligência Artificial, é o campo da ciência da computação que se dedica ao desenvolvimento de sistemas capazes de realizar tarefas que normalmente exigem inteligência humana, como reconhecimento de voz, aprendizado e tomada de decisão.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798f487-ea93-4e25-b8ea-401e2005b802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75e706e1-a9eb-4059-a1ce-030e6362c3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Para gerar a sequência de Fibonacci em Python, você pode usar uma função recursiva, uma função iterativa ou uma função com memoização. Aqui está um exemplo de uma função iterativa que gera a sequência até um número específico de termos:\\n\\n```python\\ndef fibonacci(n):\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    elif n == 2:\\n        return [0, 1]\\n\\n    fib_sequence = [0, 1]\\n    for i in range(2, n):\\n        next_fib = fib_sequence[i-1] + fib_sequence[i-2]\\n        fib_sequence.append(next_fib)\\n\\n    return fib_sequence\\n\\n# Exemplo de uso:\\nn_terms = 10\\nprint(f\"Sequência de Fibonacci até {n_terms} termos: {fibonacci(n_terms)}\")\\n```\\n\\nNeste código, a função `fibonacci` recebe um número `n` como parâmetro e retorna uma lista contendo a sequência de Fibonacci até o `n`-ésimo termo. A função começa verificando se `n` é menor ou igual a 0, retornando uma lista vazia nesse caso. Se `n` for 1, retorna uma lista contendo apenas o primeiro termo da sequência (0). Se `n` for 2, retorna uma lista contendo os dois primeiros termos (0 e 1).\\n\\nPara valores de `n` maiores que 2, a função inicializa uma lista com os dois primeiros termos da sequência e, em seguida, usa um loop para calcular os termos restantes. Cada termo é calculado como a soma dos dois termos anteriores e é adicionado à lista.\\n\\nPor fim, o exemplo de uso demonstra como chamar a função `fibonacci` com `n_terms` igual a 10 e imprimir a sequ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_prompt = \"Você é um programador experiente. Retorne o codigo requisitado e forneça explicações se necessário\"\n",
    "prompt = \"Gere um codigo python que escreva a sequência de fibonnaci.\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
    "\n",
    "output = pipe(template, **generation_args)\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913deb07-3eef-4482-9bf4-c343f27de1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff00dac5-1429-4f08-badf-276d626dc67e",
   "metadata": {},
   "source": [
    "## Formato de mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "900cf368-8aeb-44f9-93f7-135062126c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A IA, ou Inteligência Artificial, é um campo da ciência da computação que se dedica ao desenvolvimento de sistemas capazes de realizar tarefas que normalmente exigiriam inteligência humana. Esses sistemas podem aprender, raciocinar e resolver problemas, utilizando algoritmos e modelos de aprendizado de máquina. A IA pode ser aplicada em diversos domínios, como automação, análise de dados, robótica, entre outros.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"O que é IA?\"\n",
    "prompt_sys = \"Você é muito prestativo. Responsa em português\"\n",
    "\n",
    "msg = [\n",
    "    {\"role\":\"system\", \"content\": prompt_sys},\n",
    "    {\"role\":\"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "output = pipe(msg, **generation_args)\n",
    "output[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f85ce30b-d6e9-45f2-b99b-8d5e06ce3143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1. Paris, França\\n\\n2. Roma, Itália\\n\\n3. Londres, Reino Unido\\n\\n4. Berlim, Alemanha\\n\\n5. Madrid, Espanha\\n\\n6. Lisboa, Portugal\\n\\n7. Viena, Áustria\\n\\n8. Amsterdã, Países Baixos\\n\\n9. Budapeste, Hungria\\n\\n10. Estocolmo, Suécia'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Liste o nome de 10 cidades famosas na Europa\"\n",
    "prompt_sys = \"Você é muito prestativo. Responsa em português\"\n",
    "\n",
    "msg = [\n",
    "    {\"role\":\"system\", \"content\": prompt_sys},\n",
    "    {\"role\":\"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "output = pipe(msg, **generation_args)\n",
    "output[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f41e75f-535a-4e4f-9f7d-4b3e5d66bd8f",
   "metadata": {},
   "source": [
    "## Quantização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7de49677-ed47-4673-ad4a-979b5fac04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4', #tipo de quantizacao de 4 bits a ser usado. Normal Float 4\n",
    "    bnb_4bit_use_double_quant = True, #Habilita a quantizacao dupla\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 #Tipo de dado para calculo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f892245-df81-4bba-a351-2de3be925a7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_studies_dll_hugging/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_studies_dll_hugging/lib/python3.11/site-packages/transformers/modeling_utils.py:3657\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3654\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3657\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3661\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_studies_dll_hugging/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:74\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a93d01-9fd1-413c-9357-05a19a11844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"Quem foi a primeira pessoa no espaço?\")\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae75ac3-2caf-4609-a8dc-969f6e4621aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(device)\n",
    "generated_ids = model.generate(model_inputs, \n",
    "                               max_new_tokens = 1000, \n",
    "                               do_sample = True,\n",
    "                               pad_token_id=tokenizer.eos_token_id)\n",
    "decodes = tokenizer.batch_decode(generate_ids)\n",
    "res = decoded[0]\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
